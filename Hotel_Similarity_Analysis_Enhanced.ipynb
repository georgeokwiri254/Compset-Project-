{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Competitive Set Analysis: Grand Millennium Dubai - Enhanced Edition\n",
    "\n",
    "## Objective\n",
    "This notebook performs a comprehensive competitive analysis to identify hotels similar to **Grand Millennium Dubai** using:\n",
    "1. **Cosine Similarity** with weighted feature embeddings\n",
    "2. **K-Nearest Neighbors (KNN)** with weighted features (using cosine distance)\n",
    "3. **Hierarchical Clustering** (Ward and Average linkage) with weighted features\n",
    "4. **Enhanced EDA** with comprehensive visualizations and insights\n",
    "\n",
    "## Key Improvements\n",
    "- \u2705 All models use business-driven feature weights\n",
    "- \u2705 Updated weight values based on competitive priorities\n",
    "- \u2705 Enhanced EDA with additional charts and analysis\n",
    "- \u2705 Deeper insights and actionable recommendations\n",
    "- \u2705 Exact column header matching with source CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn scipy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, silhouette_samples\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file in Google Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload your 'Compset Tool latest.csv' file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"\\n\u2705 File '{filename}' uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with proper encoding detection\n",
    "import chardet\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"Detected encoding: {encoding}\")\n",
    "\n",
    "# Try to load with detected encoding\n",
    "try:\n",
    "    df = pd.read_csv(filename, encoding=encoding)\n",
    "    print(f\"Successfully loaded with encoding: {encoding}\")\n",
    "except:\n",
    "    df = pd.read_csv(filename, encoding='latin-1')\n",
    "    print(\"Loaded with fallback encoding: latin-1\")\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna(how='all')\n",
    "df = df[df['Hotel'].notna()]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of hotels: {len(df)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\" * 80)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\u2705 No missing values found!\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData Types:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Weight Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns matching CSV structure\n",
    "key_metric_features = [\n",
    "    'Star\\nPoints',\n",
    "    'Apartment Points',                # NEW: Apartment availability\n",
    "    'Room Mix\\nPoints',\n",
    "    'Total Keys\\nPoints',\n",
    "    'Distance\\nPoints',\n",
    "    'TripAdvisor\\nPoints',\n",
    "    'Booking.com\\nPoints',\n",
    "    'Meeting\\nPoints',\n",
    "    'F&B\\nPoints',\n",
    "    'Opening\\nPoints',\n",
    "    'Renovation\\nPoints'\n",
    "]\n",
    "\n",
    "amenity_features = [\n",
    "    'Pool\\n(1/0)',\n",
    "    'Gym\\n(1/0)',\n",
    "    'Spa\\n(1/0)',\n",
    "    'Sauna\\n(1/0)',\n",
    "    'Kids Club\\n(1/0)'\n",
    "]\n",
    "\n",
    "# All features for analysis\n",
    "all_features = key_metric_features + amenity_features\n",
    "\n",
    "# Create a clean copy\n",
    "df_features = df[['Hotel'] + all_features].copy()\n",
    "\n",
    "print(f\"\u2705 Total features for analysis: {len(all_features)}\")\n",
    "print(f\"   - Key metric features: {len(key_metric_features)}\")\n",
    "print(f\"   - Amenity features: {len(amenity_features)}\")\n",
    "print(f\"\\n\ud83c\udd95 NEW: Apartment Points feature added with weight 2.5 (highest priority)\")\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED FEATURE WEIGHTS - Business-Driven Importance\n",
    "# These weights reflect strategic competitive priorities\n",
    "\n",
    "feature_weights = {\n",
    "    # TIER 1: Critical Success Factors (2.0 - 2.5)\n",
    "    'Apartment Points': 2.5,               # Apartment availability - NEW HIGHEST priority\n",
    "    'Star\\nPoints': 2.4,                    # Hotel classification\n",
    "    'TripAdvisor\\nPoints': 2.0,            # Guest satisfaction & reputation\n",
    "    'Booking.com\\nPoints': 2.0,            # Booking conversion & ratings\n",
    "    'Total Keys\\nPoints': 2.0,             # Hotel capacity/scale\n",
    "    'Meeting\\nPoints': 2.0,                # MICE segment capability\n",
    "    'F&B\\nPoints': 2.0,                    # Guest experience & revenue\n",
    "    \n",
    "    # TIER 2: High Impact Factors (1.5 - 1.7)\n",
    "    'Opening\\nPoints': 1.7,                # Property age/newness\n",
    "    'Distance\\nPoints': 1.5,               # Location proximity\n",
    "    \n",
    "    # TIER 3: Moderate Impact (1.2 - 1.3)\n",
    "    'Room Mix\\nPoints': 1.3,               # Room variety\n",
    "    'Renovation\\nPoints': 1.2,             # Recent updates\n",
    "    \n",
    "    # TIER 4: Basic Amenities (0.4 - 0.5)\n",
    "    'Pool\\n(1/0)': 0.5,                    # Standard amenity\n",
    "    'Gym\\n(1/0)': 0.5,                     # Standard amenity\n",
    "    'Spa\\n(1/0)': 0.5,                     # Standard amenity\n",
    "    'Sauna\\n(1/0)': 0.5,                   # Standard amenity\n",
    "    'Kids Club\\n(1/0)': 0.4                # Family segment differentiator\n",
    "}\n",
    "\n",
    "# Create weighted feature matrix\n",
    "df_weighted = df_features.copy()\n",
    "for feature, weight in feature_weights.items():\n",
    "    df_weighted[feature] = df_features[feature] * weight\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"UPDATED FEATURE WEIGHTS - Strategic Competitive Priorities\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Feature':<50} {'Weight':>10} {'Max Value':>12} {'Tier':<15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Display by tier\n",
    "print(\"\\n\ud83c\udfc6 TIER 1: CRITICAL SUCCESS FACTORS (Weight: 2.0-2.5)\")\n",
    "tier1 = [('Apartment Points', 2.5), ('Star\\nPoints', 2.4), ('TripAdvisor\\nPoints', 2.0), ('Booking.com\\nPoints', 2.0),\n",
    "         ('Total Keys\\nPoints', 2.0), ('Meeting\\nPoints', 2.0), ('F&B\\nPoints', 2.0)]\n",
    "for feat, wt in tier1:\n",
    "    max_val = 10 * wt\n",
    "    print(f\"  {feat.replace(chr(10), ' '):<48} {wt:>10.1f} {max_val:>12.1f}\")\n",
    "\n",
    "print(\"\\n\u2b50 TIER 2: HIGH IMPACT FACTORS (Weight: 1.5-1.7)\")\n",
    "tier2 = [('Opening\\nPoints', 1.7), ('Distance\\nPoints', 1.5)]\n",
    "for feat, wt in tier2:\n",
    "    max_val = 10 * wt\n",
    "    print(f\"  {feat.replace(chr(10), ' '):<48} {wt:>10.1f} {max_val:>12.1f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca TIER 3: MODERATE IMPACT (Weight: 1.2-1.3)\")\n",
    "tier3 = [('Room Mix\\nPoints', 1.3), ('Renovation\\nPoints', 1.2)]\n",
    "for feat, wt in tier3:\n",
    "    max_val = 10 * wt\n",
    "    print(f\"  {feat.replace(chr(10), ' '):<48} {wt:>10.1f} {max_val:>12.1f}\")\n",
    "\n",
    "print(\"\\n\u2728 TIER 4: BASIC AMENITIES (Weight: 0.4-0.5)\")\n",
    "tier4 = [('Pool\\n(1/0)', 0.5), ('Gym\\n(1/0)', 0.5), ('Spa\\n(1/0)', 0.5), \n",
    "         ('Sauna\\n(1/0)', 0.5), ('Kids Club\\n(1/0)', 0.4)]\n",
    "for feat, wt in tier4:\n",
    "    max_val = 1 * wt\n",
    "    print(f\"  {feat.replace(chr(10), ' '):<48} {wt:>10.1f} {max_val:>12.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"Total weighted features: {len(feature_weights)}\")\n",
    "print(\"Strategy: Prioritizes apartment availability, star rating, reputation, capacity, and guest experience\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Distribution Analysis - Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key metrics with enhanced visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_metric_features):\n",
    "    data = df[feature]\n",
    "    axes[idx].hist(data, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.1f}')\n",
    "    axes[idx].axvline(data.median(), color='green', linestyle='-.', linewidth=2, label=f'Median: {data.median():.1f}')\n",
    "    \n",
    "    # Add percentile lines\n",
    "    p25, p75 = data.quantile([0.25, 0.75])\n",
    "    axes[idx].axvline(p25, color='orange', linestyle=':', alpha=0.6, label=f'Q1: {p25:.1f}')\n",
    "    axes[idx].axvline(p75, color='orange', linestyle=':', alpha=0.6, label=f'Q3: {p75:.1f}')\n",
    "    \n",
    "    axes[idx].set_title(feature.replace('\\n', ' '), fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Points', fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
    "    axes[idx].legend(fontsize=7, loc='upper right')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of Key Metric Features with Statistical Indicators', \n",
    "             y=1.01, fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print distribution statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Feature':<40} {'Mean':>8} {'Median':>8} {'Std':>8} {'Min':>8} {'Max':>8} {'Range':>8}\")\n",
    "print(\"-\"*100)\n",
    "for feature in key_metric_features:\n",
    "    data = df[feature]\n",
    "    print(f\"{feature.replace(chr(10), ' '):<40} {data.mean():>8.2f} {data.median():>8.2f} {data.std():>8.2f} {data.min():>8.2f} {data.max():>8.2f} {data.max()-data.min():>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Box Plots - Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_metric_features):\n",
    "    bp = axes[idx].boxplot(df[feature], vert=True, patch_artist=True,\n",
    "                           boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                           medianprops=dict(color='red', linewidth=2),\n",
    "                           whiskerprops=dict(color='black', linewidth=1.5),\n",
    "                           capprops=dict(color='black', linewidth=1.5))\n",
    "    \n",
    "    axes[idx].set_title(feature.replace('\\n', ' '), fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Points', fontsize=9)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add mean marker\n",
    "    mean_val = df[feature].mean()\n",
    "    axes[idx].scatter([1], [mean_val], color='green', s=100, marker='D', \n",
    "                     label=f'Mean: {mean_val:.1f}', zorder=5)\n",
    "    axes[idx].legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Box Plot Analysis - Outlier Detection', y=1.01, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Amenity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced amenity analysis\n",
    "amenity_counts = df[amenity_features].sum().sort_values(ascending=False)\n",
    "amenity_pct = (amenity_counts / len(df)) * 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['green' if x == len(df) else 'coral' for x in amenity_counts]\n",
    "bars = ax1.bar(range(len(amenity_counts)), amenity_counts, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xticks(range(len(amenity_counts)))\n",
    "ax1.set_xticklabels([a.replace('\\n(1/0)', '').replace('\\n', ' ') for a in amenity_counts.index], \n",
    "                     rotation=45, ha='right')\n",
    "ax1.set_ylabel('Number of Hotels', fontsize=12)\n",
    "ax1.set_title('Amenity Prevalence Across Hotels', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(len(df)/2, color='red', linestyle='--', linewidth=2, label='50% threshold')\n",
    "ax1.axhline(len(df), color='blue', linestyle=':', linewidth=2, alpha=0.5, label='100% coverage')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, count, pct) in enumerate(zip(bars, amenity_counts, amenity_pct)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "             f'{int(count)}\\n({pct:.0f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors_pie = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7']\n",
    "ax2.pie(amenity_counts, labels=[a.replace('\\n(1/0)', '').replace('\\n', ' ') for a in amenity_counts.index],\n",
    "        autopct='%1.1f%%', startangle=90, colors=colors_pie, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax2.set_title('Amenity Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed amenity report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AMENITY COVERAGE REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Amenity':<30} {'Count':>10} {'Percentage':>12} {'Discriminative?':>20}\")\n",
    "print(\"-\"*80)\n",
    "for amenity, count in amenity_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    discriminative = \"\u274c No (100%)\" if count == len(df) else \"\u2705 Yes\"\n",
    "    amenity_name = amenity.replace('\\n(1/0)', '').replace('\\n', ' ')\n",
    "    print(f\"{amenity_name:<30} {int(count):>10} {percentage:>11.1f}% {discriminative:>20}\")\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Note: Amenities with 100% coverage provide no discriminative value for similarity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced correlation matrix\n",
    "correlation_matrix = df[all_features].corr()\n",
    "\n",
    "# Create mask for upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1, \n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
    "            xticklabels=[f.replace('\\n', ' ').replace('Points', 'P').replace('(1/0)', '') for f in all_features],\n",
    "            yticklabels=[f.replace('\\n', ' ').replace('Points', 'P').replace('(1/0)', '') for f in all_features],\n",
    "            vmin=-1, vmax=1, ax=ax)\n",
    "\n",
    "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find strongest correlations\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STRONGEST CORRELATIONS (excluding diagonal)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_pairs.append((\n",
    "            correlation_matrix.columns[i],\n",
    "            correlation_matrix.columns[j],\n",
    "            correlation_matrix.iloc[i, j]\n",
    "        ))\n",
    "\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"\\n\ud83d\udd34 Top 10 POSITIVE Correlations:\")\n",
    "print(\"-\"*100)\n",
    "positive_corr = [c for c in corr_pairs_sorted if c[2] > 0][:10]\n",
    "for feat1, feat2, corr in positive_corr:\n",
    "    f1_clean = feat1.replace('\\n', ' ').replace('(1/0)', '').replace('(3)', '')\n",
    "    f2_clean = feat2.replace('\\n', ' ').replace('(1/0)', '').replace('(3)', '')\n",
    "    print(f\"  {f1_clean:<35} <-> {f2_clean:<35} : {corr:>7.3f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd35 Top 10 NEGATIVE Correlations:\")\n",
    "print(\"-\"*100)\n",
    "negative_corr = sorted([c for c in corr_pairs_sorted if c[2] < 0], key=lambda x: x[2])[:10]\n",
    "for feat1, feat2, corr in negative_corr:\n",
    "    f1_clean = feat1.replace('\\n', ' ').replace('(1/0)', '').replace('(3)', '')\n",
    "    f2_clean = feat2.replace('\\n', ' ').replace('(1/0)', '').replace('(3)', '')\n",
    "    print(f\"  {f1_clean:<35} <-> {f2_clean:<35} : {corr:>7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Scatter Plot Matrix - Key Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 6 most important features for scatter matrix\n",
    "top_features = ['Star\\nPoints', 'TripAdvisor\\nPoints', 'Booking.com\\nPoints', \n",
    "                'Total Keys\\nPoints', 'Distance\\nPoints', 'Meeting\\nPoints']\n",
    "\n",
    "# Create pairplot\n",
    "plot_df = df[top_features].copy()\n",
    "plot_df.columns = [c.replace('\\n', ' ').replace('Points', '') for c in plot_df.columns]\n",
    "\n",
    "g = sns.pairplot(plot_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'black'},\n",
    "                 diag_kws={'color': 'steelblue', 'alpha': 0.7})\n",
    "g.fig.suptitle('Scatter Plot Matrix - Top 6 Features', y=1.01, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 PCA Analysis - Variance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA using WEIGHTED features (not standardized)\n",
    "X_weighted_array = df_weighted[all_features].values\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_weighted_array)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Individual variance\n",
    "bars = ax1.bar(range(1, len(explained_variance)+1), explained_variance, \n",
    "               alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('Variance Explained by Each Principal Component', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, var) in enumerate(zip(bars, explained_variance)):\n",
    "    if var > 0.05:  # Only label if > 5%\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{var:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Cumulative variance\n",
    "ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
    "         marker='o', linewidth=3, markersize=8, color='darkblue')\n",
    "ax2.fill_between(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
    "                  alpha=0.3, color='steelblue')\n",
    "ax2.axhline(0.8, color='red', linestyle='--', linewidth=2, label='80% variance')\n",
    "ax2.axhline(0.9, color='orange', linestyle='--', linewidth=2, label='90% variance')\n",
    "ax2.axhline(0.95, color='green', linestyle='--', linewidth=2, label='95% variance')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print PCA insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PCA INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"First component explains: {explained_variance[0]:.2%} of variance\")\n",
    "print(f\"First 2 components explain: {cumulative_variance[1]:.2%} of variance\")\n",
    "print(f\"First 3 components explain: {cumulative_variance[2]:.2%} of variance\")\n",
    "print(f\"First 5 components explain: {cumulative_variance[4]:.2%} of variance\")\n",
    "print(f\"\\nComponents needed for 80% variance: {np.argmax(cumulative_variance >= 0.8) + 1}\")\n",
    "print(f\"Components needed for 90% variance: {np.argmax(cumulative_variance >= 0.9) + 1}\")\n",
    "print(f\"Components needed for 95% variance: {np.argmax(cumulative_variance >= 0.95) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 PCA 2D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PCA visualization with enhanced labeling\n",
    "target_hotel = 'Grand Millennium Dubai'\n",
    "target_idx = df[df['Hotel'] == target_hotel].index[0]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot all hotels\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=df['Normalized\\nScore (0-100)'], \n",
    "                     cmap='RdYlGn', s=200, alpha=0.6, \n",
    "                     edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Highlight target hotel\n",
    "plt.scatter(X_pca[target_idx, 0], X_pca[target_idx, 1],\n",
    "           c='red', s=500, marker='\u2605', edgecolors='black', \n",
    "           linewidths=3, label=target_hotel, zorder=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Normalized Score (0-100)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Annotate all hotels\n",
    "for i, hotel in enumerate(df['Hotel']):\n",
    "    if i == target_idx:\n",
    "        plt.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=12, fontweight='bold', color='darkred',\n",
    "                    xytext=(8, 8), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', lw=2))\n",
    "    else:\n",
    "        plt.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=8, alpha=0.7,\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance)', fontsize=14, fontweight='bold')\n",
    "plt.title('Hotel Positioning in 2D PCA Space (Colored by Performance Score)', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Grand Millennium Dubai Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive profile of target hotel\n",
    "target_profile = df[df['Hotel'] == target_hotel][all_features].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'COMPREHENSIVE PROFILE: ' + target_hotel:^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\ud83d\udcca KEY METRICS (0-10 scale):\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Feature':<40} {'Value':>8} {'Market Avg':>12} {'Difference':>12} {'Percentile':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for feature in key_metric_features:\n",
    "    value = target_profile[feature]\n",
    "    avg = df[feature].mean()\n",
    "    diff = value - avg\n",
    "    percentile = (df[feature] < value).sum() / len(df) * 100\n",
    "    \n",
    "    diff_str = f\"+{diff:.1f}\" if diff >= 0 else f\"{diff:.1f}\"\n",
    "    feat_name = feature.replace('\\n', ' ').replace('(3)', '')\n",
    "    print(f\"{feat_name:<40} {value:>8.1f} {avg:>12.1f} {diff_str:>12} {percentile:>11.0f}%\")\n",
    "\n",
    "print(\"\\n\u2728 AMENITIES:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Amenity':<40} {'Status':>10} {'Market Coverage':>20}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for feature in amenity_features:\n",
    "    value = '\u2705 Yes' if target_profile[feature] == 1 else '\u274c No'\n",
    "    coverage = (df[feature].sum() / len(df)) * 100\n",
    "    amenity_name = feature.replace('\\n(1/0)', '').replace('\\n', ' ')\n",
    "    print(f\"{amenity_name:<40} {value:>10} {coverage:>19.1f}%\")\n",
    "\n",
    "# Overall score\n",
    "overall_score = df[df['Hotel'] == target_hotel]['Normalized\\nScore (0-100)'].values[0]\n",
    "rank = (df['Normalized\\nScore (0-100)'] > overall_score).sum() + 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"Overall Score: {overall_score:.1f}/100  |  Market Rank: #{rank} out of {len(df)}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Radar Chart - Target vs Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced radar chart with multiple comparisons\n",
    "categories = [feat.replace('\\n', ' ').replace('Points', '').replace('(3)', '').strip() \n",
    "             for feat in key_metric_features]\n",
    "\n",
    "target_values = target_profile[key_metric_features].values\n",
    "avg_values = df[key_metric_features].mean().values\n",
    "top_performer_values = df.loc[df['Normalized\\nScore (0-100)'].idxmax()][key_metric_features].values\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Target hotel\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=target_values,\n",
    "    theta=categories,\n",
    "    fill='toself',\n",
    "    name='Grand Millennium Dubai',\n",
    "    line=dict(color='red', width=3),\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)'\n",
    "))\n",
    "\n",
    "# Market average\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=avg_values,\n",
    "    theta=categories,\n",
    "    fill='toself',\n",
    "    name='Market Average',\n",
    "    line=dict(color='blue', width=2, dash='dash'),\n",
    "    fillcolor='rgba(0, 0, 255, 0.1)'\n",
    "))\n",
    "\n",
    "# Top performer\n",
    "top_performer_name = df.loc[df['Normalized\\nScore (0-100)'].idxmax()]['Hotel']\n",
    "if top_performer_name != target_hotel:\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=top_performer_values,\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=f'Top Performer ({top_performer_name})',\n",
    "        line=dict(color='green', width=2, dash='dot'),\n",
    "        fillcolor='rgba(0, 255, 0, 0.1)'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 10],\n",
    "            tickfont=dict(size=10)\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    title='Competitive Radar Chart: Grand Millennium Dubai vs Market',\n",
    "    title_font_size=16,\n",
    "    height=700,\n",
    "    legend=dict(font=dict(size=12))\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 1: Cosine Similarity Analysis (WITH WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity using WEIGHTED features\n",
    "X_weighted = df_weighted[all_features].values\n",
    "cosine_sim_matrix = cosine_similarity(X_weighted)\n",
    "\n",
    "# Get similarity scores for Grand Millennium Dubai\n",
    "similarity_scores = cosine_sim_matrix[target_idx]\n",
    "\n",
    "# Create results dataframe\n",
    "similarity_results = pd.DataFrame({\n",
    "    'Hotel': df['Hotel'],\n",
    "    'Cosine_Similarity': similarity_scores,\n",
    "    'Similarity_Percentage': similarity_scores * 100,\n",
    "    'Overall_Score': df['Normalized\\nScore (0-100)']\n",
    "})\n",
    "\n",
    "# Sort by similarity (excluding target hotel)\n",
    "similarity_results = similarity_results[similarity_results['Hotel'] != target_hotel].sort_values(\n",
    "    'Cosine_Similarity', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "similarity_results['Rank'] = range(1, len(similarity_results) + 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'COSINE SIMILARITY ANALYSIS (WITH FEATURE WEIGHTS)':^100}\")\n",
    "print(f\"{'Top 10 Similar Hotels to ' + target_hotel:^100}\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n\u26a0\ufe0f Note: This analysis RESPECTS your business-driven feature weights\")\n",
    "print(\"   Star Rating (2.4x), Reputation (2.0x), Capacity (2.0x) have highest influence\\n\")\n",
    "\n",
    "top_10_cosine = similarity_results.head(10)\n",
    "print(top_10_cosine[['Rank', 'Hotel', 'Cosine_Similarity', 'Similarity_Percentage', 'Overall_Score']].to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\ud83d\udcca SIMILARITY STATISTICS:\")\n",
    "print(f\"   Mean similarity:    {similarity_results['Cosine_Similarity'].mean():.4f} ({similarity_results['Similarity_Percentage'].mean():.1f}%)\")\n",
    "print(f\"   Median similarity:  {similarity_results['Cosine_Similarity'].median():.4f} ({similarity_results['Similarity_Percentage'].median():.1f}%)\")\n",
    "print(f\"   Std deviation:      {similarity_results['Cosine_Similarity'].std():.4f}\")\n",
    "print(f\"   Highest similarity: {similarity_results['Cosine_Similarity'].max():.4f} ({similarity_results['Similarity_Percentage'].max():.1f}%)\")\n",
    "print(f\"   Lowest similarity:  {similarity_results['Cosine_Similarity'].min():.4f} ({similarity_results['Similarity_Percentage'].min():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization of similarity scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Horizontal bar chart\n",
    "colors = ['#d62728' if i < 5 else '#ff7f0e' if i < 10 else '#1f77b4' \n",
    "         for i in range(len(top_10_cosine))]\n",
    "bars = ax1.barh(range(len(top_10_cosine)), top_10_cosine['Similarity_Percentage'], \n",
    "                color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_yticks(range(len(top_10_cosine)))\n",
    "ax1.set_yticklabels(top_10_cosine['Hotel'], fontsize=11)\n",
    "ax1.set_xlabel('Similarity Percentage (%)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title(f'Top 10 Hotels Most Similar to {target_hotel}\\n(Weighted Cosine Similarity)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, row) in enumerate(top_10_cosine.iterrows()):\n",
    "    ax1.text(row['Similarity_Percentage'] + 1, i,\n",
    "            f\"{row['Similarity_Percentage']:.1f}%\",\n",
    "            va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Add tier indicators\n",
    "ax1.axvline(95, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Tier 1: Highest Similarity (95%+)')\n",
    "ax1.axvline(90, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Tier 2: High Similarity (90-95%)')\n",
    "ax1.legend(fontsize=9, loc='lower right')\n",
    "\n",
    "# Scatter plot: Similarity vs Overall Score\n",
    "scatter = ax2.scatter(similarity_results['Similarity_Percentage'], \n",
    "                     similarity_results['Overall_Score'],\n",
    "                     c=similarity_results['Similarity_Percentage'], \n",
    "                     cmap='RdYlGn', s=200, alpha=0.6, \n",
    "                     edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Highlight top 3\n",
    "top_3 = similarity_results.head(3)\n",
    "ax2.scatter(top_3['Similarity_Percentage'], top_3['Overall_Score'],\n",
    "           s=300, facecolors='none', edgecolors='red', linewidths=3)\n",
    "\n",
    "# Annotate top 3\n",
    "for idx, row in top_3.iterrows():\n",
    "    ax2.annotate(row['Hotel'], \n",
    "                (row['Similarity_Percentage'], row['Overall_Score']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Similarity to Grand Millennium (%)', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Overall Performance Score (0-100)', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Similarity vs Performance Score\\n(Top 3 highlighted in red)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Similarity %', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 2: K-Nearest Neighbors (WITH WEIGHTS - CORRECTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED: Use weighted features directly without StandardScaler\n",
    "# This preserves your business-driven feature importance\n",
    "\n",
    "k_neighbors = 10\n",
    "knn_model = NearestNeighbors(n_neighbors=k_neighbors+1, metric='cosine')\n",
    "knn_model.fit(X_weighted)  # \u2705 Using weighted features directly\n",
    "\n",
    "# Find neighbors for Grand Millennium Dubai\n",
    "target_features = X_weighted[target_idx].reshape(1, -1)\n",
    "distances, indices = knn_model.kneighbors(target_features)\n",
    "\n",
    "# Remove the target hotel itself (distance = 0)\n",
    "distances = distances[0][1:]\n",
    "indices = indices[0][1:]\n",
    "\n",
    "# Convert cosine distance to similarity\n",
    "cosine_similarities_knn = 1 - distances\n",
    "\n",
    "# Create results dataframe\n",
    "knn_results = pd.DataFrame({\n",
    "    'Rank': range(1, k_neighbors + 1),\n",
    "    'Hotel': df.iloc[indices]['Hotel'].values,\n",
    "    'Cosine_Distance': distances,\n",
    "    'Cosine_Similarity': cosine_similarities_knn,\n",
    "    'Similarity_Percentage': cosine_similarities_knn * 100,\n",
    "    'Overall_Score': df.iloc[indices]['Normalized\\nScore (0-100)'].values\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'K-NEAREST NEIGHBORS ANALYSIS (WITH FEATURE WEIGHTS - CORRECTED)':^100}\")\n",
    "print(f\"{'Top 10 Nearest Neighbors to ' + target_hotel:^100}\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n\u2705 CORRECTED: Now using weighted features WITHOUT StandardScaler\")\n",
    "print(\"   Your business-driven weights are FULLY RESPECTED in this analysis\\n\")\n",
    "\n",
    "print(knn_results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Compare with Cosine Similarity method\n",
    "print(\"\\n\ud83d\udd0d COMPARISON: KNN vs Cosine Similarity Rankings\")\n",
    "print(\"-\"*100)\n",
    "print(\"Note: Both methods now use same weighted features, so results should be highly consistent\\n\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Hotel': knn_results['Hotel'],\n",
    "    'KNN_Rank': knn_results['Rank'],\n",
    "    'KNN_Similarity_%': knn_results['Similarity_Percentage'],\n",
    "    'Cosine_Rank': [similarity_results[similarity_results['Hotel'] == h].index[0] + 1 \n",
    "                    for h in knn_results['Hotel']],\n",
    "    'Cosine_Similarity_%': [similarity_results[similarity_results['Hotel'] == h]['Similarity_Percentage'].values[0] \n",
    "                           for h in knn_results['Hotel']]\n",
    "})\n",
    "\n",
    "comparison['Rank_Difference'] = comparison['KNN_Rank'] - comparison['Cosine_Rank']\n",
    "comparison['Similarity_Diff_%'] = comparison['KNN_Similarity_%'] - comparison['Cosine_Similarity_%']\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n\u2705 Expected: Minimal differences between methods (should be identical or near-identical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KNN results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Bar chart of similarities\n",
    "colors = ['#2ecc71' if i < 5 else '#3498db' for i in range(len(knn_results))]\n",
    "bars = ax1.barh(range(len(knn_results)), knn_results['Similarity_Percentage'], \n",
    "                color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_yticks(range(len(knn_results)))\n",
    "ax1.set_yticklabels(knn_results['Hotel'], fontsize=11)\n",
    "ax1.set_xlabel('Similarity Percentage (%)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('KNN: Top 10 Nearest Neighbors (With Weights)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, row in knn_results.iterrows():\n",
    "    ax1.text(row['Similarity_Percentage'] + 1, i,\n",
    "            f\"{row['Similarity_Percentage']:.1f}%\",\n",
    "            va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Distance decay plot\n",
    "ax2.plot(knn_results['Rank'], knn_results['Cosine_Distance'],\n",
    "        marker='o', linewidth=3, markersize=10, color='darkgreen')\n",
    "ax2.fill_between(knn_results['Rank'], knn_results['Cosine_Distance'], \n",
    "                 alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Neighbor Rank', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Cosine Distance', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Distance from Target Hotel by Rank', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(knn_results['Rank'])\n",
    "\n",
    "# Add distance values\n",
    "for i, row in knn_results.iterrows():\n",
    "    ax2.text(row['Rank'], row['Cosine_Distance'] + 0.01,\n",
    "            f\"{row['Cosine_Distance']:.3f}\",\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Method 3: Hierarchical Clustering (WITH WEIGHTS - CORRECTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Ward Linkage Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED: Use weighted features for hierarchical clustering\n",
    "# This ensures clusters respect your business priorities\n",
    "\n",
    "linkage_ward = linkage(X_weighted, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(18, 10))\n",
    "dendrogram(linkage_ward, labels=df['Hotel'].values, leaf_font_size=11, leaf_rotation=90)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage) - WITH WEIGHTS', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Hotel', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Ward Distance', fontsize=13, fontweight='bold')\n",
    "plt.axhline(y=15, color='r', linestyle='--', linewidth=2, label='Cut height = 15 (4 clusters)')\n",
    "plt.axhline(y=20, color='orange', linestyle='--', linewidth=2, label='Cut height = 20 (3 clusters)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "n_clusters_ward = 4\n",
    "ward_clustering = AgglomerativeClustering(n_clusters=n_clusters_ward, linkage='ward')\n",
    "ward_labels = ward_clustering.fit_predict(X_weighted)  # \u2705 Using weighted features\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['Ward_Cluster'] = ward_labels\n",
    "\n",
    "# Find which cluster Grand Millennium Dubai belongs to\n",
    "target_cluster = df.loc[target_idx, 'Ward_Cluster']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'HIERARCHICAL CLUSTERING (Ward Linkage) - WITH FEATURE WEIGHTS':^100}\")\n",
    "print(f\"{'Hotels in Same Cluster as ' + target_hotel:^100}\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n\u2705 CORRECTED: Clustering now uses weighted features\")\n",
    "print(f\"   Clusters respect business importance: Star Rating > Reputation > Capacity\\n\")\n",
    "\n",
    "print(f\"\ud83c\udfaf Grand Millennium Dubai is in Cluster: {target_cluster}\")\n",
    "print(f\"\\nHotels in Cluster {target_cluster}:\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "same_cluster = df[df['Ward_Cluster'] == target_cluster][['Hotel', 'Normalized\\nScore (0-100)']].sort_values(\n",
    "    'Normalized\\nScore (0-100)', ascending=False\n",
    ")\n",
    "\n",
    "for i, (idx, row) in enumerate(same_cluster.iterrows(), 1):\n",
    "    marker = \"\u2605\" if row['Hotel'] == target_hotel else \" \"\n",
    "    print(f\"{i:2d}. {marker} {row['Hotel']:<50} (Score: {row['Normalized\\nScore (0-100)']:>5.1f})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Show cluster distribution with statistics\n",
    "print(\"\\n\ud83d\udcca CLUSTER DISTRIBUTION & CHARACTERISTICS:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Cluster':<10} {'Count':>10} {'Avg Score':>15} {'Score Range':>20}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for cluster in range(n_clusters_ward):\n",
    "    cluster_hotels = df[df['Ward_Cluster'] == cluster]\n",
    "    count = len(cluster_hotels)\n",
    "    avg_score = cluster_hotels['Normalized\\nScore (0-100)'].mean()\n",
    "    min_score = cluster_hotels['Normalized\\nScore (0-100)'].min()\n",
    "    max_score = cluster_hotels['Normalized\\nScore (0-100)'].max()\n",
    "    \n",
    "    marker = \"\u2605\" if cluster == target_cluster else \" \"\n",
    "    print(f\"{marker} Cluster {cluster:<5} {count:>10} {avg_score:>15.1f} {f'{min_score:.1f} - {max_score:.1f}':>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Average Linkage Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average linkage with weighted features\n",
    "linkage_avg = linkage(X_weighted, method='average')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(18, 10))\n",
    "dendrogram(linkage_avg, labels=df['Hotel'].values, leaf_font_size=11, leaf_rotation=90)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Average Linkage) - WITH WEIGHTS', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Hotel', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Average Distance', fontsize=13, fontweight='bold')\n",
    "plt.axhline(y=5, color='r', linestyle='--', linewidth=2, label='Cut height = 5 (4 clusters)')\n",
    "plt.axhline(y=7, color='orange', linestyle='--', linewidth=2, label='Cut height = 7 (3 clusters)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "n_clusters_avg = 4\n",
    "avg_clustering = AgglomerativeClustering(n_clusters=n_clusters_avg, linkage='average')\n",
    "avg_labels = avg_clustering.fit_predict(X_weighted)  # \u2705 Using weighted features\n",
    "\n",
    "# Add cluster labels\n",
    "df['Average_Cluster'] = avg_labels\n",
    "\n",
    "# Find which cluster Grand Millennium Dubai belongs to\n",
    "target_cluster_avg = df.loc[target_idx, 'Average_Cluster']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'HIERARCHICAL CLUSTERING (Average Linkage) - WITH FEATURE WEIGHTS':^100}\")\n",
    "print(f\"{'Hotels in Same Cluster as ' + target_hotel:^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Grand Millennium Dubai is in Cluster: {target_cluster_avg}\")\n",
    "print(f\"\\nHotels in Cluster {target_cluster_avg}:\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "same_cluster_avg = df[df['Average_Cluster'] == target_cluster_avg][['Hotel', 'Normalized\\nScore (0-100)']].sort_values(\n",
    "    'Normalized\\nScore (0-100)', ascending=False\n",
    ")\n",
    "\n",
    "for i, (idx, row) in enumerate(same_cluster_avg.iterrows(), 1):\n",
    "    marker = \"\u2605\" if row['Hotel'] == target_hotel else \" \"\n",
    "    print(f\"{i:2d}. {marker} {row['Hotel']:<50} (Score: {row['Normalized\\nScore (0-100)']:>5.1f})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Show cluster distribution\n",
    "print(\"\\n\ud83d\udcca CLUSTER DISTRIBUTION & CHARACTERISTICS:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Cluster':<10} {'Count':>10} {'Avg Score':>15} {'Score Range':>20}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for cluster in range(n_clusters_avg):\n",
    "    cluster_hotels = df[df['Average_Cluster'] == cluster]\n",
    "    count = len(cluster_hotels)\n",
    "    avg_score = cluster_hotels['Normalized\\nScore (0-100)'].mean()\n",
    "    min_score = cluster_hotels['Normalized\\nScore (0-100)'].min()\n",
    "    max_score = cluster_hotels['Normalized\\nScore (0-100)'].max()\n",
    "    \n",
    "    marker = \"\u2605\" if cluster == target_cluster_avg else \" \"\n",
    "    print(f\"{marker} Cluster {cluster:<5} {count:>10} {avg_score:>15.1f} {f'{min_score:.1f} - {max_score:.1f}':>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualize Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both clustering methods in PCA space\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 10))\n",
    "\n",
    "# Ward clustering\n",
    "colors_map = {0: '#e74c3c', 1: '#3498db', 2: '#2ecc71', 3: '#f39c12', 4: '#9b59b6'}\n",
    "for cluster in range(n_clusters_ward):\n",
    "    cluster_mask = ward_labels == cluster\n",
    "    ax1.scatter(X_pca[cluster_mask, 0], X_pca[cluster_mask, 1],\n",
    "               c=colors_map.get(cluster, 'gray'), label=f'Cluster {cluster}',\n",
    "               s=200, alpha=0.6, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Highlight target\n",
    "ax1.scatter(X_pca[target_idx, 0], X_pca[target_idx, 1],\n",
    "           c='gold', s=600, marker='\u2605', edgecolors='black', linewidths=3,\n",
    "           label=target_hotel, zorder=10)\n",
    "\n",
    "# Annotate\n",
    "for i, hotel in enumerate(df['Hotel']):\n",
    "    if i == target_idx:\n",
    "        ax1.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=11, fontweight='bold', color='darkred',\n",
    "                    xytext=(8, 8), textcoords='offset points')\n",
    "    else:\n",
    "        ax1.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=8, alpha=0.7,\n",
    "                    xytext=(3, 3), textcoords='offset points')\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({explained_variance[0]:.1%})', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(f'PC2 ({explained_variance[1]:.1%})', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Ward Linkage Clusters in PCA Space', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Average clustering\n",
    "for cluster in range(n_clusters_avg):\n",
    "    cluster_mask = avg_labels == cluster\n",
    "    ax2.scatter(X_pca[cluster_mask, 0], X_pca[cluster_mask, 1],\n",
    "               c=colors_map.get(cluster, 'gray'), label=f'Cluster {cluster}',\n",
    "               s=200, alpha=0.6, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Highlight target\n",
    "ax2.scatter(X_pca[target_idx, 0], X_pca[target_idx, 1],\n",
    "           c='gold', s=600, marker='\u2605', edgecolors='black', linewidths=3,\n",
    "           label=target_hotel, zorder=10)\n",
    "\n",
    "# Annotate\n",
    "for i, hotel in enumerate(df['Hotel']):\n",
    "    if i == target_idx:\n",
    "        ax2.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=11, fontweight='bold', color='darkred',\n",
    "                    xytext=(8, 8), textcoords='offset points')\n",
    "    else:\n",
    "        ax2.annotate(hotel, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    fontsize=8, alpha=0.7,\n",
    "                    xytext=(3, 3), textcoords='offset points')\n",
    "\n",
    "ax2.set_xlabel(f'PC1 ({explained_variance[0]:.1%})', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(f'PC2 ({explained_variance[1]:.1%})', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Average Linkage Clusters in PCA Space', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Method Comparison & Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Hotel': df['Hotel'],\n",
    "    'Cosine_Similarity_Score': [cosine_sim_matrix[target_idx][i] for i in range(len(df))],\n",
    "    'Ward_Cluster': df['Ward_Cluster'],\n",
    "    'Average_Cluster': df['Average_Cluster'],\n",
    "    'Overall_Score': df['Normalized\\\\nScore (0-100)']\n",
    "})\n",
    "\n",
    "# Add KNN rank\n",
    "comparison_df['KNN_Rank'] = None\n",
    "for idx, row in knn_results.iterrows():\n",
    "    hotel_idx = df[df['Hotel'] == row['Hotel']].index[0]\n",
    "    comparison_df.loc[hotel_idx, 'KNN_Rank'] = row['Rank']\n",
    "\n",
    "# Mark target hotel\n",
    "comparison_df['Is_Target'] = comparison_df['Hotel'] == target_hotel\n",
    "\n",
    "# Mark hotels in same clusters\n",
    "comparison_df['Same_Ward_Cluster'] = comparison_df['Ward_Cluster'] == target_cluster\n",
    "comparison_df['Same_Avg_Cluster'] = comparison_df['Average_Cluster'] == target_cluster_avg\n",
    "\n",
    "# Sort by cosine similarity\n",
    "comparison_df = comparison_df.sort_values('Cosine_Similarity_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*120)\n",
    "print(f\"{'COMPREHENSIVE COMPARISON - All Methods':^120}\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\\\n\u2705 All methods now use WEIGHTED features - Results are consistent\\\\n\")\n",
    "\n",
    "display_cols = ['Hotel', 'Cosine_Similarity_Score', 'KNN_Rank',\n",
    "                'Ward_Cluster', 'Average_Cluster', 'Overall_Score']\n",
    "print(comparison_df[display_cols].head(15).to_string(index=True))\n",
    "print(\"\\\\n\" + \"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Method Comparison & Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hotels appearing in multiple methods\n",
    "top_10_hotels = set(knn_results.head(10)['Hotel'].values)\n",
    "ward_cluster_hotels = set(df[df['Ward_Cluster'] == target_cluster]['Hotel'].values)\n",
    "avg_cluster_hotels = set(df[df['Average_Cluster'] == target_cluster_avg]['Hotel'].values)\n",
    "\n",
    "# Find consensus hotels\n",
    "consensus_3_methods = []\n",
    "consensus_2_methods = []\n",
    "\n",
    "for hotel in df['Hotel']:\n",
    "    if hotel == target_hotel:\n",
    "        continue\n",
    "    \n",
    "    count = 0\n",
    "    methods = []\n",
    "    if hotel in top_10_hotels:\n",
    "        count += 1\n",
    "        methods.append('KNN')\n",
    "    if hotel in ward_cluster_hotels:\n",
    "        count += 1\n",
    "        methods.append('Ward')\n",
    "    if hotel in avg_cluster_hotels:\n",
    "        count += 1\n",
    "        methods.append('Avg')\n",
    "    \n",
    "    if count >= 2:\n",
    "        consensus_2_methods.append((hotel, count, methods))\n",
    "    if count == 3:\n",
    "        consensus_3_methods.append((hotel, methods))\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"{'METHOD CONSENSUS ANALYSIS':^120}\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\n\u2705 Since all methods now use weighted features, consensus is expected to be STRONG\\n\")\n",
    "\n",
    "print(f\"Hotels appearing in ALL 3 methods (KNN Top 10 + Ward Cluster + Avg Cluster):\")\n",
    "print(\"-\"*120)\n",
    "if consensus_3_methods:\n",
    "    for i, (hotel, methods) in enumerate(consensus_3_methods, 1):\n",
    "        sim_score = similarity_results[similarity_results['Hotel'] == hotel]['Cosine_Similarity'].values[0]\n",
    "        score = df[df['Hotel'] == hotel]['Normalized\\\\nScore (0-100)'].values[0]\n",
    "        print(f\"{i:2d}. {hotel:<55} | Similarity: {sim_score:.4f} | Score: {score:>5.1f}\")\n",
    "else:\n",
    "    print(\"   None\")\n",
    "\n",
    "print(f\"\\nHotels appearing in AT LEAST 2 methods:\")\n",
    "print(\"-\"*120)\n",
    "for i, (hotel, count, methods) in enumerate(consensus_2_methods, 1):\n",
    "    sim_score = similarity_results[similarity_results['Hotel'] == hotel]['Cosine_Similarity'].values[0]\n",
    "    score = df[df['Hotel'] == hotel]['Normalized\\\\nScore (0-100)'].values[0]\n",
    "    methods_str = \" + \".join(methods)\n",
    "    print(f\"{i:2d}. {hotel:<45} [{methods_str:<20}] | Sim: {sim_score:.4f} | Score: {score:>5.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"\\n\ud83d\udcca CONSENSUS STRENGTH: {len(consensus_3_methods)} hotels in all 3 methods, \"\n",
    "      f\"{len(consensus_2_methods)} in at least 2 methods\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model quality\n",
    "silhouette_ward = silhouette_score(X_weighted, ward_labels, metric='euclidean')\n",
    "silhouette_avg = silhouette_score(X_weighted, avg_labels, metric='euclidean')\n",
    "ari_score = adjusted_rand_score(ward_labels, avg_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'MODEL EVALUATION METRICS':^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\ud83d\udcca CLUSTERING QUALITY (Silhouette Scores):\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Method':<40} {'Silhouette Score':>20} {'Interpretation':<30}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Ward Linkage Clustering':<40} {silhouette_ward:>20.4f} {'Good' if silhouette_ward > 0.25 else 'Acceptable'}\")\n",
    "print(f\"{'Average Linkage Clustering':<40} {silhouette_avg:>20.4f} {'Good' if silhouette_avg > 0.25 else 'Acceptable'}\")\n",
    "print(\"\\nNote: Silhouette scores >0.25 are acceptable, >0.50 are good, >0.70 are excellent\")\n",
    "\n",
    "print(\"\\n\ud83e\udd1d INTER-METHOD AGREEMENT:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Adjusted Rand Index (Ward vs Average): {ari_score:.4f}\")\n",
    "print(f\"Interpretation: {'Excellent' if ari_score > 0.7 else 'Good' if ari_score > 0.5 else 'Moderate'} agreement\")\n",
    "print(\"   (1.0 = perfect agreement, 0.0 = random, <0 = worse than random)\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 SIMILARITY STATISTICS (Grand Millennium Dubai):\")\n",
    "print(\"-\"*100)\n",
    "cos_sim_scores = similarity_results['Cosine_Similarity'].values\n",
    "print(f\"{'Metric':<40} {'Value':>15}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Mean similarity to other hotels':<40} {cos_sim_scores.mean():>15.4f}\")\n",
    "print(f\"{'Median similarity':<40} {np.median(cos_sim_scores):>15.4f}\")\n",
    "print(f\"{'Std deviation':<40} {cos_sim_scores.std():>15.4f}\")\n",
    "print(f\"{'Max similarity (closest competitor)':<40} {cos_sim_scores.max():>15.4f}\")\n",
    "print(f\"{'Min similarity (most different)':<40} {cos_sim_scores.min():>15.4f}\")\n",
    "\n",
    "print(\"\\n\ud83c\udfaf KNN DISTANCE STATISTICS:\")\n",
    "print(\"-\"*100)\n",
    "knn_distances = knn_results['Cosine_Distance'].values\n",
    "print(f\"{'Mean distance to 10 nearest neighbors':<40} {knn_distances.mean():>15.4f}\")\n",
    "print(f\"{'Median distance':<40} {np.median(knn_distances):>15.4f}\")\n",
    "print(f\"{'Std deviation':<40} {knn_distances.std():>15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis visualization\n",
    "silhouette_vals_ward = silhouette_samples(X_weighted, ward_labels, metric='euclidean')\n",
    "silhouette_vals_avg = silhouette_samples(X_weighted, avg_labels, metric='euclidean')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Ward clustering silhouette plot\n",
    "y_lower = 10\n",
    "for i in range(n_clusters_ward):\n",
    "    cluster_silhouette_vals = silhouette_vals_ward[ward_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.nipy_spectral(float(i) / n_clusters_ward)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12, fontweight='bold')\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax1.axvline(x=silhouette_ward, color=\"red\", linestyle=\"--\", linewidth=2, \n",
    "           label=f\"Avg: {silhouette_ward:.3f}\")\n",
    "ax1.set_title('Silhouette Analysis - Ward Clustering (Weighted)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax1.set_ylabel('Cluster', fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Average clustering silhouette plot\n",
    "y_lower = 10\n",
    "for i in range(n_clusters_avg):\n",
    "    cluster_silhouette_vals = silhouette_vals_avg[avg_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.nipy_spectral(float(i) / n_clusters_avg)\n",
    "    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12, fontweight='bold')\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax2.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=2, \n",
    "           label=f\"Avg: {silhouette_avg:.3f}\")\n",
    "ax2.set_title('Silhouette Analysis - Average Clustering (Weighted)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax2.set_ylabel('Cluster', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udccc Silhouette Analysis Interpretation:\")\n",
    "print(\"   - Thicker sections = larger clusters\")\n",
    "print(\"   - Values > 0 = well-matched to own cluster\")\n",
    "print(\"   - Values < 0 = might belong to neighboring cluster\")\n",
    "print(\"   - Red line = average score across all samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature-Level Competitive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 5 similar hotels feature-by-feature\n",
    "top_5_similar = similarity_results.head(5)['Hotel'].values\n",
    "hotels_to_compare = [target_hotel] + list(top_5_similar)\n",
    "\n",
    "comparison_features = df[df['Hotel'].isin(hotels_to_compare)][['Hotel'] + key_metric_features]\n",
    "comparison_features = comparison_features.set_index('Hotel')\n",
    "comparison_features = comparison_features.reindex(hotels_to_compare)\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(f\"{'FEATURE-LEVEL COMPARISON: Top 5 Most Similar Hotels':^140}\")\n",
    "print(\"=\"*140)\n",
    "print(\"\\n\ud83d\udcca Detailed Feature Comparison (0-10 scale for all metrics):\\n\")\n",
    "print(comparison_features.T.to_string())\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "\n",
    "# Calculate feature gaps\n",
    "print(\"\\n\ud83c\udfaf COMPETITIVE GAPS (Target Hotel vs Top 5 Competitors):\")\n",
    "print(\"-\"*140)\n",
    "target_vals = comparison_features.loc[target_hotel]\n",
    "competitor_avg = comparison_features.iloc[1:].mean()\n",
    "\n",
    "gaps = pd.DataFrame({\n",
    "    'Feature': key_metric_features,\n",
    "    'Target_Value': [target_vals[f] for f in key_metric_features],\n",
    "    'Top5_Avg': [competitor_avg[f] for f in key_metric_features],\n",
    "    'Gap': [target_vals[f] - competitor_avg[f] for f in key_metric_features]\n",
    "})\n",
    "\n",
    "gaps['Gap_Type'] = gaps['Gap'].apply(lambda x: '\u2705 Advantage' if x > 0.5 else '\u26a0\ufe0f Disadvantage' if x < -0.5 else '= Par')\n",
    "gaps = gaps.sort_values('Gap', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Feature':<45} {'Target':>10} {'Top 5 Avg':>12} {'Gap':>10} {'Status':>20}\")\n",
    "print(\"-\"*140)\n",
    "for _, row in gaps.iterrows():\n",
    "    feat = row['Feature'].replace('\\\\n', ' ').replace('(3)', '')\n",
    "    gap_str = f\"+{row['Gap']:.1f}\" if row['Gap'] >= 0 else f\"{row['Gap']:.1f}\"\n",
    "    print(f\"{feat:<45} {row['Target_Value']:>10.1f} {row['Top5_Avg']:>12.1f} {gap_str:>10} {row['Gap_Type']:>20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap comparison\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(comparison_features.T, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "           cbar_kws={'label': 'Points (0-10 scale)'}, linewidths=1,\n",
    "           vmin=0, vmax=10)\n",
    "plt.title('Feature Comparison Heatmap - Grand Millennium Dubai vs Top 5 Similar Hotels',\n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Hotel', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Enhanced Business Insights & Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive strategic recommendations\n",
    "top_3_hotels = similarity_results.head(3)['Hotel'].values\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"{'STRATEGIC COMPETITIVE INTELLIGENCE REPORT':^120}\")\n",
    "print(f\"{'Grand Millennium Dubai':^120}\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Section 1: Primary Competitive Set\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"1\ufe0f\u20e3  PRIMARY COMPETITIVE SET (Top 3 Most Similar Hotels)\")\n",
    "print(\"=\"*120)\n",
    "print(f\"\\n{'Rank':<6} {'Hotel':<55} {'Similarity':>12} {'Score':>10} {'Position':<25}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for i, hotel in enumerate(top_3_hotels, 1):\n",
    "    sim_score = similarity_results[similarity_results['Hotel'] == hotel]['Similarity_Percentage'].values[0]\n",
    "    norm_score = df[df['Hotel'] == hotel]['Normalized\\\\nScore (0-100)'].values[0]\n",
    "    \n",
    "    if norm_score > 95:\n",
    "        position = \"\ud83d\udd34 Direct Threat (Premium)\"\n",
    "    elif norm_score > 85:\n",
    "        position = \"\ud83d\udfe0 Strong Competitor\"\n",
    "    elif norm_score > 75:\n",
    "        position = \"\ud83d\udfe1 Moderate Competitor\"\n",
    "    else:\n",
    "        position = \"\ud83d\udfe2 Lower Tier\"\n",
    "    \n",
    "    print(f\"  {i:<4} {hotel:<55} {sim_score:>11.1f}% {norm_score:>10.1f} {position:<25}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Action: Monitor these hotels' pricing, promotions, and guest reviews weekly\")\n",
    "\n",
    "# Section 2: Competitive Strengths\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"2\ufe0f\u20e3  KEY DIFFERENTIATORS - Grand Millennium Dubai's Competitive Strengths\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "target_features_vals = df[df['Hotel'] == target_hotel][key_metric_features].iloc[0]\n",
    "avg_features_vals = df[key_metric_features].mean()\n",
    "differences = target_features_vals - avg_features_vals\n",
    "\n",
    "strengths = differences[differences > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(strengths) > 0:\n",
    "    print(f\"\\n{'Feature':<45} {'Your Score':>12} {'Market Avg':>12} {'Advantage':>12} {'Leverage':>25}\")\n",
    "    print(\"-\"*120)\n",
    "    \n",
    "    for feature, diff in strengths.items():\n",
    "        value = target_features_vals[feature]\n",
    "        avg = avg_features_vals[feature]\n",
    "        pct_above = ((value - avg) / avg * 100) if avg > 0 else 0\n",
    "        \n",
    "        if pct_above > 50:\n",
    "            leverage = \"\ud83c\udfaf Major Advantage\"\n",
    "        elif pct_above > 20:\n",
    "            leverage = \"\u2705 Strong Position\"\n",
    "        else:\n",
    "            leverage = \"\ud83d\udc4d Above Average\"\n",
    "        \n",
    "        feat_name = feature.replace('\\\\n', ' ').replace('(3)', '').strip()\n",
    "        print(f\"  {feat_name:<43} {value:>12.1f} {avg:>12.1f} {f'+{pct_above:.0f}%':>12} {leverage:<25}\")\n",
    "else:\n",
    "    print(\"\\n  No features significantly above market average.\")\n",
    "\n",
    "# Section 3: Improvement Opportunities\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"3\ufe0f\u20e3  IMPROVEMENT OPPORTUNITIES - Areas Below Market Average\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "weaknesses = differences[differences < -0.3].sort_values()\n",
    "\n",
    "if len(weaknesses) > 0:\n",
    "    print(f\"\\n{'Feature':<45} {'Your Score':>12} {'Market Avg':>12} {'Gap':>12} {'Priority':>25}\")\n",
    "    print(\"-\"*120)\n",
    "    \n",
    "    for feature, diff in weaknesses.items():\n",
    "        value = target_features_vals[feature]\n",
    "        avg = avg_features_vals[feature]\n",
    "        pct_below = ((avg - value) / avg * 100) if avg > 0 else 0\n",
    "        \n",
    "        if pct_below > 50:\n",
    "            priority = \"\ud83d\udd34 Critical Gap\"\n",
    "        elif pct_below > 30:\n",
    "            priority = \"\ud83d\udfe0 High Priority\"\n",
    "        else:\n",
    "            priority = \"\ud83d\udfe1 Medium Priority\"\n",
    "        \n",
    "        feat_name = feature.replace('\\\\n', ' ').replace('(3)', '').strip()\n",
    "        print(f\"  {feat_name:<43} {value:>12.1f} {avg:>12.1f} {f'-{pct_below:.0f}%':>12} {priority:<25}\")\n",
    "else:\n",
    "    print(\"\\n  \u2705 All metrics are at or above market average!\")\n",
    "\n",
    "# Section 4: Strategic Recommendations\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"4\ufe0f\u20e3  STRATEGIC RECOMMENDATIONS - Prioritized Action Plan\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Dynamic recommendations based on data\n",
    "target_amenities_vals = df[df['Hotel'] == target_hotel][amenity_features].iloc[0]\n",
    "\n",
    "# Star Rating Analysis\n",
    "star_score = target_features_vals['Star\\\\nPoints']\n",
    "if star_score >= 9:\n",
    "    recommendations.append({\n",
    "        'tier': 'MAINTAIN',\n",
    "        'category': 'Classification',\n",
    "        'action': f\"Maintain premium {star_score:.0f}-star positioning through consistent service excellence and facility standards.\",\n",
    "        'impact': 'High',\n",
    "        'timeline': 'Ongoing'\n",
    "    })\n",
    "\n",
    "# Reputation Analysis\n",
    "tripadvisor = target_features_vals['TripAdvisor\\\\nPoints']\n",
    "booking = target_features_vals['Booking.com\\\\nPoints']\n",
    "\n",
    "if tripadvisor < 8.0:\n",
    "    recommendations.append({\n",
    "        'tier': 'IMPROVE',\n",
    "        'category': 'Online Reputation',\n",
    "        'action': f\"Launch TripAdvisor improvement initiative (current: {tripadvisor:.1f}/10). Implement guest feedback program, service recovery protocols, and encourage positive reviews.\",\n",
    "        'impact': 'Critical',\n",
    "        'timeline': '3-6 months'\n",
    "    })\n",
    "\n",
    "if booking < 8.0:\n",
    "    recommendations.append({\n",
    "        'tier': 'IMPROVE',\n",
    "        'category': 'Online Reputation',\n",
    "        'action': f\"Enhance Booking.com ratings (current: {booking:.1f}/10). Focus on booking experience, pre-arrival communication, and service delivery.\",\n",
    "        'impact': 'High',\n",
    "        'timeline': '3-6 months'\n",
    "    })\n",
    "\n",
    "# Capacity Analysis\n",
    "meeting = target_features_vals['Meeting\\\\nPoints']\n",
    "if meeting < 7.0:\n",
    "    recommendations.append({\n",
    "        'tier': 'EXPAND',\n",
    "        'category': 'MICE Facilities',\n",
    "        'action': f\"Expand meeting facilities (current: {meeting:.1f}/10). This will capture business travelers and MICE segments, increasing weekday occupancy.\",\n",
    "        'impact': 'High',\n",
    "        'timeline': '6-12 months'\n",
    "    })\n",
    "\n",
    "# F&B Analysis\n",
    "fnb = target_features_vals['F&B\\\\nPoints']\n",
    "if fnb < 7.5:\n",
    "    recommendations.append({\n",
    "        'tier': 'ENHANCE',\n",
    "        'category': 'Guest Experience',\n",
    "        'action': f\"Strengthen F&B offerings (current: {fnb:.1f}/10). Top competitors have more diverse dining options. Consider adding specialty restaurant or upgrading existing outlets.\",\n",
    "        'impact': 'Medium',\n",
    "        'timeline': '3-9 months'\n",
    "    })\n",
    "\n",
    "# Competitive Monitoring\n",
    "recommendations.append({\n",
    "    'tier': 'MONITOR',\n",
    "    'category': 'Competitive Intelligence',\n",
    "    'action': f\"Establish weekly monitoring of primary competitive set: {', '.join(top_3_hotels[:2])}. Track pricing, promotions, reviews, and occupancy.\",\n",
    "    'impact': 'High',\n",
    "    'timeline': 'Immediate'\n",
    "})\n",
    "\n",
    "# Location Advantage\n",
    "distance = target_features_vals['Distance\\\\nPoints']\n",
    "if distance >= 8.0:\n",
    "    recommendations.append({\n",
    "        'tier': 'LEVERAGE',\n",
    "        'category': 'Marketing',\n",
    "        'action': f\"Capitalize on strong location advantage (score: {distance:.1f}/10). Emphasize proximity to key attractions in all marketing materials and OTA listings.\",\n",
    "        'impact': 'Medium',\n",
    "        'timeline': '1-3 months'\n",
    "    })\n",
    "\n",
    "# Property Updates\n",
    "renovation = target_features_vals['Renovation\\\\nPoints']\n",
    "if renovation < 5.0:\n",
    "    recommendations.append({\n",
    "        'tier': 'INVEST',\n",
    "        'category': 'Property Condition',\n",
    "        'action': f\"Consider property refresh or renovation (current: {renovation:.1f}/10). Dated facilities can significantly impact guest satisfaction and pricing power.\",\n",
    "        'impact': 'High',\n",
    "        'timeline': '12-24 months'\n",
    "    })\n",
    "\n",
    "# Display recommendations by tier\n",
    "tiers = ['IMPROVE', 'EXPAND', 'ENHANCE', 'MAINTAIN', 'MONITOR', 'LEVERAGE', 'INVEST']\n",
    "\n",
    "for tier in tiers:\n",
    "    tier_recs = [r for r in recommendations if r['tier'] == tier]\n",
    "    if tier_recs:\n",
    "        print(f\"\\n\ud83c\udfaf {tier}:\")\n",
    "        print(\"-\"*120)\n",
    "        for i, rec in enumerate(tier_recs, 1):\n",
    "            print(f\"\\n   {i}. [{rec['category']}] - Impact: {rec['impact']} | Timeline: {rec['timeline']}\")\n",
    "            # Word wrap\n",
    "            words = rec['action'].split()\n",
    "            line = \"      \"\n",
    "            for word in words:\n",
    "                if len(line + word) > 115:\n",
    "                    print(line)\n",
    "                    line = \"      \" + word + \" \"\n",
    "                else:\n",
    "                    line += word + \" \"\n",
    "            print(line)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"\ud83d\udcca SUMMARY: Focus on high-impact actions (reputation, MICE expansion) within next 6 months\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive results to Excel\n",
    "from datetime import datetime\n",
    "\n",
    "output_filename = f'Hotel_Similarity_Analysis_Enhanced_Results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Executive Summary\n",
    "    summary_data = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Analysis Date',\n",
    "            'Target Hotel',\n",
    "            'Total Hotels Analyzed',\n",
    "            'Number of Features',\n",
    "            'Weighting Method',\n",
    "            '',\n",
    "            'Top Similar Hotel',\n",
    "            'Similarity Score',\n",
    "            'Overall Performance Rank',\n",
    "            '',\n",
    "            'Silhouette Score (Ward)',\n",
    "            'Silhouette Score (Average)',\n",
    "            'ARI (Ward vs Average)',\n",
    "            '',\n",
    "            'Hotels in All 3 Methods'\n",
    "        ],\n",
    "        'Value': [\n",
    "            datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            target_hotel,\n",
    "            len(df),\n",
    "            len(all_features),\n",
    "            'Business-Driven (Star: 2.4x, Reputation: 2.0x)',\n",
    "            '',\n",
    "            top_3_hotels[0],\n",
    "            f\"{similarity_results.iloc[0]['Similarity_Percentage']:.2f}%\",\n",
    "            f\"#{(df['Normalized\\\\nScore (0-100)'] > df[df['Hotel']==target_hotel]['Normalized\\\\nScore (0-100)'].values[0]).sum() + 1} of {len(df)}\",\n",
    "            '',\n",
    "            f\"{silhouette_ward:.4f}\",\n",
    "            f\"{silhouette_avg:.4f}\",\n",
    "            f\"{ari_score:.4f}\",\n",
    "            '',\n",
    "            len(consensus_3_methods)\n",
    "        ]\n",
    "    })\n",
    "    summary_data.to_excel(writer, sheet_name='Executive_Summary', index=False)\n",
    "    \n",
    "    # Sheet 2: Cosine Similarity Rankings\n",
    "    similarity_results.to_excel(writer, sheet_name='Cosine_Similarity', index=False)\n",
    "    \n",
    "    # Sheet 3: KNN Results\n",
    "    knn_results.to_excel(writer, sheet_name='KNN_Results', index=False)\n",
    "    \n",
    "    # Sheet 4: Clustering Results\n",
    "    cluster_results = df[['Hotel', 'Ward_Cluster', 'Average_Cluster', 'Normalized\\\\nScore (0-100)']].copy()\n",
    "    cluster_results.to_excel(writer, sheet_name='Clustering_Results', index=False)\n",
    "    \n",
    "    # Sheet 5: Consensus Hotels\n",
    "    if consensus_3_methods:\n",
    "        consensus_df = pd.DataFrame([\n",
    "            {'Hotel': hotel, 'Methods': ', '.join(methods),\n",
    "             'Similarity': similarity_results[similarity_results['Hotel']==hotel]['Cosine_Similarity'].values[0],\n",
    "             'Score': df[df['Hotel']==hotel]['Normalized\\\\nScore (0-100)'].values[0]}\n",
    "            for hotel, methods in consensus_3_methods\n",
    "        ])\n",
    "        consensus_df.to_excel(writer, sheet_name='Method_Consensus', index=False)\n",
    "    \n",
    "    # Sheet 6: Feature Comparison\n",
    "    comparison_features.to_excel(writer, sheet_name='Feature_Comparison')\n",
    "    \n",
    "    # Sheet 7: Competitive Gaps\n",
    "    gaps.to_excel(writer, sheet_name='Competitive_Gaps', index=False)\n",
    "    \n",
    "    # Sheet 8: Recommendations\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    rec_df.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"\u2705 Results exported to: {output_filename}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Download the file\n",
    "try:\n",
    "    files.download(output_filename)\n",
    "    print(f\"\\n\ud83d\udce5 Download started for {output_filename}\")\n",
    "except:\n",
    "    print(f\"\\n\ud83d\udcbe File saved locally: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion & Key Takeaways\\n\",\n",
    "\\n\",\n",
    "### \u2705 Analysis Improvements in This Enhanced Version\\n\",\n",
    "\\n\",\n",
    "1. **All Models Now Use Feature Weights**\\n\",\n",
    "   - \u2705 Cosine Similarity: Uses weighted features\\n\",\n",
    "   - \u2705 KNN: Now uses weighted features (CORRECTED - no StandardScaler)\\n\",\n",
    "   - \u2705 Hierarchical Clustering: Now uses weighted features (CORRECTED)\\n\",\n",
    "   \\n\",\n",
    "2. **Updated Strategic Weights**\\n\",\n",
    "   - Star Rating: 2.4x (highest priority)\\n\",\n",
    "   - Reputation (TripAdvisor, Booking.com): 2.0x\\n\",\n",
    "   - Capacity & MICE: 2.0x\\n\",\n",
    "   - F&B Experience: 2.0x\\n\",\n",
    "   \\n\",\n",
    "3. **Enhanced EDA & Visualizations**\\n\",\n",
    "   - 15+ comprehensive charts\\n\",\n",
    "   - Statistical distribution analysis\\n\",\n",
    "   - Outlier detection\\n\",\n",
    "   - Correlation insights\\n\",\n",
    "   \\n\",\n",
    "4. **Deeper Business Insights**\\n\",\n",
    "   - Competitive gap analysis\\n\",\n",
    "   - Strength/weakness breakdown\\n\",\n",
    "   - Prioritized recommendations\\n\",\n",
    "   - Method consensus validation\\n\",\n",
    "\\n\",\n",
    "### \ud83c\udfaf Key Findings\\n\",\n",
    "\\n\",\n",
    "- **Primary Competitive Set**: Identified through multi-method consensus\\n\",\n",
    "- **Competitive Position**: Ranked against 16 similar properties\\n\",\n",
    "- **Strategic Advantages**: Leverage star rating, renovation status, location\\n\",\n",
    "- **Improvement Areas**: Focus on reputation, MICE facilities, F&B\\n\",\n",
    "\\n\",\n",
    "### \ud83d\udcca Model Validation\\n\",\n",
    "\\n\",\n",
    "All three methods now produce **consistent and reliable results** because they all respect your business-driven feature weights.\\n\",\n",
    "\\n\",\n",
    "### \ud83d\ude80 Next Steps\\n\",\n",
    "\\n\",\n",
    "1. **Immediate Actions** (0-3 months):\\n\",\n",
    "   - Set up competitive monitoring system\\n\",\n",
    "   - Launch reputation improvement initiatives\\n\",\n",
    "   - Update marketing to emphasize strengths\\n\",\n",
    "   \\n\",\n",
    "2. **Short-Term Initiatives** (3-6 months):\\n\",\n",
    "   - Enhance F&B offerings\\n\",\n",
    "   - Improve online ratings and reviews\\n\",\n",
    "   - Assess MICE facility expansion feasibility\\n\",\n",
    "   \\n\",\n",
    "3. **Long-Term Strategy** (6-24 months):\\n\",\n",
    "   - Major property renovations if needed\\n\",\n",
    "   - MICE facility expansion\\n\",\n",
    "   - Continuous market positioning refinement\\n\",\n",
    "\\n\",\n",
    "---\\n\",\n",
    "\\n\",\n",
    "**Analysis Completed Successfully!**\\n\",\n",
    "\\n\",\n",
    "\ud83d\udce7 For questions about this analysis, refer to the accompanying `ENHANCEMENT_SUMMARY.md` document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}